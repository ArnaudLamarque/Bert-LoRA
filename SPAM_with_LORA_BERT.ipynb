{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85c15f62",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85c15f62",
        "outputId": "5e1b1789-e6d4-4c06-919e-92b09b335cf8",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.11/dist-packages (1.7.1)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (2.0.2)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (24.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (2.6.0+cu124)\n",
            "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (0.14.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.2.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.13.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->torchmetrics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "from tabulate import tabulate\n",
        "from tqdm import trange\n",
        "import random\n",
        "from torchmetrics.classification import Recall, Accuracy, AUROC, Precision"
      ],
      "metadata": {
        "id": "6GisTlddPg5T"
      },
      "id": "6GisTlddPg5T",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FILL_IN = \"FILL_IN\""
      ],
      "metadata": {
        "id": "4ed8K7sR8Flv"
      },
      "id": "4ed8K7sR8Flv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3678adfa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3678adfa",
        "outputId": "4d3d9d7e-43b7-4ee6-95c6-673694c4e64f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-12 03:56:06--  https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified\n",
            "Saving to: ‘smsspamcollection.zip.4’\n",
            "\n",
            "smsspamcollection.z     [   <=>              ] 198.65K   307KB/s    in 0.6s    \n",
            "\n",
            "2025-05-12 03:56:07 (307 KB/s) - ‘smsspamcollection.zip.4’ saved [203415]\n",
            "\n",
            "Archive:  smsspamcollection.zip\n",
            "  inflating: SMSSpamCollection       \n",
            "  inflating: readme                  \n"
          ]
        }
      ],
      "source": [
        "!wget 'https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip'\n",
        "!unzip -o smsspamcollection.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d892553d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d892553d",
        "outputId": "37a301fe-45a6-4eea-f362-09fe590d6f63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  smsspamcollection.zip\n",
            "  inflating: SMSSpamCollection       \n",
            "  inflating: readme                  \n"
          ]
        }
      ],
      "source": [
        "!unzip -o smsspamcollection.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd3d5e84",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dd3d5e84",
        "outputId": "c359216e-e929-4c76-db95-7d0ce54a3a29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ham\tGo until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n",
            "ham\tOk lar... Joking wif u oni...\n",
            "spam\tFree entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\n",
            "ham\tU dun say so early hor... U c already then say...\n",
            "ham\tNah I don't think he goes to usf, he lives around here though\n",
            "spam\tFreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, £1.50 to rcv\n",
            "ham\tEven my brother is not like to speak with me. They treat me like aids patent.\n",
            "ham\tAs per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your callertune for all Callers. Press *9 to copy your friends Callertune\n",
            "spam\tWINNER!! As a valued network customer you have been selected to receivea £900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.\n",
            "spam\tHad your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with camera for Free! Call The Mobile Update Co FREE on 08002986030\n"
          ]
        }
      ],
      "source": [
        "!head -10 SMSSpamCollection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b12150d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9b12150d",
        "outputId": "b6c36611-171b-497c-db44-5e79762640dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-12 03:56:08--  https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified\n",
            "Saving to: ‘smsspamcollection.zip.5’\n",
            "\n",
            "smsspamcollection.z     [   <=>              ] 198.65K   309KB/s    in 0.6s    \n",
            "\n",
            "2025-05-12 03:56:09 (309 KB/s) - ‘smsspamcollection.zip.5’ saved [203415]\n",
            "\n",
            "Archive:  smsspamcollection.zip\n",
            "  inflating: SMSSpamCollection       \n",
            "  inflating: readme                  \n"
          ]
        }
      ],
      "source": [
        "!wget 'https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip'\n",
        "!unzip -o smsspamcollection.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98a736a1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "98a736a1",
        "outputId": "974ab62c-e5f0-44d5-fddd-5fdd89a16384"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   label                                               text\n",
              "0      0  Go until jurong point, crazy.. Available only ...\n",
              "1      0                    Ok lar... Joking wif u oni...\\n\n",
              "2      1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
              "3      0  U dun say so early hor... U c already then say...\n",
              "4      0  Nah I don't think he goes to usf, he lives aro..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-346ffefb-6fc3-4a09-8240-b464aedcd5ed\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>Ok lar... Joking wif u oni...\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "      \n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-346ffefb-6fc3-4a09-8240-b464aedcd5ed')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "      \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-346ffefb-6fc3-4a09-8240-b464aedcd5ed button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-346ffefb-6fc3-4a09-8240-b464aedcd5ed');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "  \n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "file_path = 'SMSSpamCollection'\n",
        "df = pd.DataFrame({'label':int(), 'text':str()}, index = [])\n",
        "with open(file_path) as f:\n",
        "    for line in f.readlines():\n",
        "        split = line.split('\\t')\n",
        "        df = pd.concat([\n",
        "                df,\n",
        "                pd.DataFrame.from_dict({\n",
        "                    'label': [1 if split[0] == 'spam' else 0],\n",
        "                    'text': [split[1]]\n",
        "                })\n",
        "            ],\n",
        "            ignore_index=True\n",
        "        )\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79a5cb02",
      "metadata": {
        "id": "79a5cb02"
      },
      "outputs": [],
      "source": [
        "text = df.text.values\n",
        "labels = df.label.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dce193a4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dce193a4",
        "outputId": "cad0b5db-a0f9-4059-9408-69bb8a242b5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Get the bert-base-uncased tokenizer and set lower case to True\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c00a3e7e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c00a3e7e",
        "outputId": "a87dee82-1c21-43f1-d68c-ffe574b5be64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "╒══════════╤═════════════╕\n",
            "│ Tokens   │   Token IDs │\n",
            "╞══════════╪═════════════╡\n",
            "│ hm       │       20287 │\n",
            "├──────────┼─────────────┤\n",
            "│ ##ph     │        8458 │\n",
            "├──────────┼─────────────┤\n",
            "│ .        │        1012 │\n",
            "├──────────┼─────────────┤\n",
            "│ go       │        2175 │\n",
            "├──────────┼─────────────┤\n",
            "│ head     │        2132 │\n",
            "├──────────┼─────────────┤\n",
            "│ ,        │        1010 │\n",
            "├──────────┼─────────────┤\n",
            "│ big      │        2502 │\n",
            "├──────────┼─────────────┤\n",
            "│ ball     │        3608 │\n",
            "├──────────┼─────────────┤\n",
            "│ ##er     │        2121 │\n",
            "├──────────┼─────────────┤\n",
            "│ .        │        1012 │\n",
            "╘══════════╧═════════════╛\n"
          ]
        }
      ],
      "source": [
        "def print_rand_sentence():\n",
        "    '''Displays the tokens and respective IDs of a random text sample'''\n",
        "    index = random.randint(0, len(text)-1)\n",
        "    table = np.array([tokenizer.tokenize(text[index]),\n",
        "                    tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text[index]))]).T\n",
        "    print(tabulate(table,\n",
        "                 headers = ['Tokens', 'Token IDs'],\n",
        "                 tablefmt = 'fancy_grid'))\n",
        "\n",
        "print_rand_sentence()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e85d0895",
      "metadata": {
        "id": "e85d0895"
      },
      "outputs": [],
      "source": [
        "token_id = []\n",
        "attention_masks = []\n",
        "\n",
        "def preprocessing(input_text, tokenizer):\n",
        "  '''\n",
        "  Returns <class transformers.tokenization_utils_base.BatchEncoding> with the following fields:\n",
        "    - input_ids: list of token ids\n",
        "    - token_type_ids: list of token type ids\n",
        "    - attention_mask: list of indices (0,1) specifying which tokens should considered by the model (return_attention_mask = True).\n",
        "  '''\n",
        "  return tokenizer.encode_plus(\n",
        "        input_text,\n",
        "        max_length=32,\n",
        "        truncation=True,\n",
        "        padding='max_length',\n",
        "        return_tensors='pt',\n",
        "        return_attention_mask=True,\n",
        "        return_token_type_ids=False\n",
        "    )\n",
        "\n",
        "for sample in text:\n",
        "    encoding_dict = preprocessing(sample, tokenizer)\n",
        "    token_id.append(encoding_dict['input_ids'])\n",
        "    attention_masks.append(encoding_dict['attention_mask'])\n",
        "\n",
        "\n",
        "token_id = torch.cat(token_id, dim = 0)\n",
        "attention_masks = torch.cat(attention_masks, dim = 0)\n",
        "labels =  torch.tensor(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "222e06a7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "222e06a7",
        "outputId": "34739b96-5b63-4b3a-b256-9d7ee87d208f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "╒════════════╤═════════════╤══════════════════╕\n",
            "│ Tokens     │   Token IDs │   Attention Mask │\n",
            "╞════════════╪═════════════╪══════════════════╡\n",
            "│ [CLS]      │         101 │                1 │\n",
            "├────────────┼─────────────┼──────────────────┤\n",
            "│ no         │        2053 │                1 │\n",
            "├────────────┼─────────────┼──────────────────┤\n",
            "│ my         │        2026 │                1 │\n",
            "├────────────┼─────────────┼──────────────────┤\n",
            "│ blankets   │       15019 │                1 │\n",
            "├────────────┼─────────────┼──────────────────┤\n",
            "│ are        │        2024 │                1 │\n",
            "├────────────┼─────────────┼──────────────────┤\n",
            "│ sufficient │        7182 │                1 │\n",
            "├────────────┼─────────────┼──────────────────┤\n",
            "│ ,          │        1010 │                1 │\n",
            "├────────────┼─────────────┼──────────────────┤\n",
            "│ th         │       16215 │                1 │\n",
            "├────────────┼─────────────┼──────────────────┤\n",
            "│ ##x        │        2595 │                1 │\n",
            "├────────────┼─────────────┼──────────────────┤\n",
            "│ [SEP]      │         102 │                1 │\n",
            "├────────────┼─────────────┼──────────────────┤\n",
            "│ [PAD]      │           0 │                0 │\n",
            "├────────────┼─────────────┼──────────────────┤\n",
            "│ [PAD]      │           0 │                0 │\n",
            "├────────────┼─────────────┼──────────────────┤\n",
            "│ [PAD]      │           0 │                0 │\n",
            "├────────────┼─────────────┼──────────────────┤\n",
            "│ [PAD]      │           0 │                0 │\n",
            "├────────────┼─────────────┼──────────────────┤\n",
            "│ [PAD]      │           0 │                0 │\n",
            "├────────────┼─────────────┼──────────────────┤\n",
            "│ [PAD]      │           0 │                0 │\n",
            "├────────────┼─────────────┼──────────────────┤\n",
            "│ [PAD]      │           0 │                0 │\n",
            "├────────────┼─────────────┼──────────────────┤\n",
            "│ [PAD]      │           0 │                0 │\n",
            "├────────────┼─────────────┼──────────────────┤\n",
            "│ [PAD]      │           0 │                0 │\n",
            "├────────────┼─────────────┼──────────────────┤\n",
            "│ [PAD]      │           0 │                0 │\n",
            "├────────────┼─────────────┼──────────────────┤\n",
            "│ [PAD]      │           0 │                0 │\n",
            "├────────────┼─────────────┼──────────────────┤\n",
            "│ [PAD]      │           0 │                0 │\n",
            "├────────────┼─────────────┼──────────────────┤\n",
            "│ [PAD]      │           0 │                0 │\n",
            "├────────────┼─────────────┼──────────────────┤\n",
            "│ [PAD]      │           0 │                0 │\n",
            "├────────────┼─────────────┼──────────────────┤\n",
            "│ [PAD]      │           0 │                0 │\n",
            "├────────────┼─────────────┼──────────────────┤\n",
            "│ [PAD]      │           0 │                0 │\n",
            "├────────────┼─────────────┼──────────────────┤\n",
            "│ [PAD]      │           0 │                0 │\n",
            "├────────────┼─────────────┼──────────────────┤\n",
            "│ [PAD]      │           0 │                0 │\n",
            "├────────────┼─────────────┼──────────────────┤\n",
            "│ [PAD]      │           0 │                0 │\n",
            "├────────────┼─────────────┼──────────────────┤\n",
            "│ [PAD]      │           0 │                0 │\n",
            "├────────────┼─────────────┼──────────────────┤\n",
            "│ [PAD]      │           0 │                0 │\n",
            "├────────────┼─────────────┼──────────────────┤\n",
            "│ [PAD]      │           0 │                0 │\n",
            "╘════════════╧═════════════╧══════════════════╛\n"
          ]
        }
      ],
      "source": [
        "def print_rand_sentence_encoding():\n",
        "    '''Displays tokens, token IDs and attention mask of a random text sample'''\n",
        "    index = random.randint(0, len(text) - 1)\n",
        "    tokens = tokenizer.tokenize(tokenizer.decode(token_id[index]))\n",
        "    token_ids = [i.numpy() for i in token_id[index]]\n",
        "    attention = [i.numpy() for i in attention_masks[index]]\n",
        "    table = np.array([tokens, token_ids, attention]).T\n",
        "    print(\n",
        "        tabulate(\n",
        "            table,\n",
        "            headers = ['Tokens', 'Token IDs', 'Attention Mask'],\n",
        "            tablefmt = 'fancy_grid')\n",
        "    )\n",
        "\n",
        "print_rand_sentence_encoding()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1c2c10b",
      "metadata": {
        "id": "e1c2c10b"
      },
      "outputs": [],
      "source": [
        "val_ratio = 0.2\n",
        "# Recommended batch size: 16, 32. See: https://arxiv.org/pdf/1810.04805.pdf\n",
        "batch_size = 16\n",
        "\n",
        "# Indices of the train and validation splits stratified by labels\n",
        "# Use train_test_split\n",
        "train_idx, val_idx = train_test_split(np.arange(len(labels)), test_size=val_ratio, stratify=labels, random_state=42)\n",
        "\n",
        "# Train and validation sets\n",
        "# Set to TensorDataset\n",
        "train_set = TensorDataset(token_id[train_idx], attention_masks[train_idx], labels[train_idx])\n",
        "\n",
        "val_set = TensorDataset(token_id[val_idx], attention_masks[val_idx], labels[val_idx])\n",
        "\n",
        "# Prepare DataLoader\n",
        "train_dataloader = DataLoader(train_set, sampler=RandomSampler(train_set), batch_size=batch_size)\n",
        "\n",
        "validation_dataloader = DataLoader(val_set, sampler=SequentialSampler(val_set), batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the LoRA specific layers."
      ],
      "metadata": {
        "id": "zqd2vdeT1Ywe"
      },
      "id": "zqd2vdeT1Ywe"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0de762ca",
      "metadata": {
        "id": "0de762ca"
      },
      "outputs": [],
      "source": [
        "# Define a LoRA Layer which has A, B and alpha parameters\n",
        "class LoRALayer(torch.nn.Module):\n",
        "  def __init__(self, in_dim, out_dim, rank, alpha):\n",
        "    super().__init__()\n",
        "    # Initialize A to be a parameter matrix of dimension in_dim by rank\n",
        "    self.A = nn.Parameter(torch.randn(in_dim, rank))\n",
        "    # Initialize all the elements of A via kaiming_uniform with a equal to sqrt(5)\n",
        "    nn.init.kaiming_uniform_(self.A, a=math.sqrt(5))\n",
        "    # Initialize B to be a zero parameter matrix of the appropriate dimensions\n",
        "    self.B = nn.Parameter(torch.zeros(rank, out_dim))\n",
        "    self.alpha = alpha\n",
        "    self.rank = rank\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Pass x through the LoRA layer and return the new x\n",
        "       x = (x @ self.A @ self.B)*self.alpha/self.rank\n",
        "       return x\n",
        "\n",
        "# Define a class LoRALinear which has a linear layer and a LoRA layer on top\n",
        "class LoRALinear(torch.nn.Module):\n",
        "  def __init__(self, linear, rank, alpha):\n",
        "    super().__init__()\n",
        "    self.linear = linear\n",
        "    self.lora = LoRALayer(linear.in_features, linear.out_features, rank, alpha)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Pass x through the linear layer and also the lora layer\n",
        "    return self.linear(x) + self.lora(x)\n",
        "\n",
        "def lora_linear_replace(model, rank, alpha):\n",
        "  # Use model.named_children to go through all layers and if the layer is Linear replace that layer with LoRALinear\n",
        "  for name, module in model.named_children():\n",
        "    # If the module is linear, replace the module in the model with a LoRA layer\n",
        "    if isinstance(module, nn.Linear):\n",
        "      setattr(model, name, LoRALinear(module, rank, alpha))\n",
        "    else:\n",
        "      # Alterntively, recursively apply the same function to child modules so that other Linear layers get replaced\n",
        "      lora_linear_replace(module, rank, alpha)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40bcf8c4",
      "metadata": {
        "id": "40bcf8c4"
      },
      "source": [
        "### Load specific versions of the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f85fc88e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f85fc88e",
        "outputId": "cfa52e51-7509-4ad8-d503-4a619f0cb51c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1538\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "# Load the BertForSequenceClassification model\n",
        "# Do not ouput the attentions and all hidden states\n",
        "\n",
        "model = model = BertForSequenceClassification.from_pretrained(\n",
        "    'bert-base-uncased',\n",
        "    num_labels=2,\n",
        "    output_attentions=False,\n",
        "    output_hidden_states=False\n",
        "    )\n",
        "\n",
        "# Turn off all gradients of the model to start\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Set to True if LORA is used; if False, fine_tune flag will be used to decide if you fine tune the entire model or just parts\n",
        "use_lora  = False\n",
        "# If this is False, turn off gradients\n",
        "fine_tune = False\n",
        "# Set total_parameters to 0; this will count the number of parameters in each case\n",
        "total_parameters = 0\n",
        "\n",
        "if use_lora:\n",
        "  # Use the lora_linear to attach a LoRA layer to each linear later of the original BERT model\n",
        "  lora_linear_replace(model, rank=8, alpha=16)\n",
        "  # Get the total number of parameters with gradients\n",
        "  total_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "else:\n",
        "  # If fine_tune is off, turn off gradients for all layers other than classifier\n",
        "  if not fine_tune:\n",
        "  # Turn off all gradients; count just the 'classifier' layer which is the only one that has gradients\n",
        "      for name, param in model.named_parameters():\n",
        "          if 'classifier' in name:\n",
        "              param.requires_grad = True\n",
        "          else:\n",
        "              param.requires_grad = False\n",
        "      total_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "  else:\n",
        "    for name, param in model.named_parameters():\n",
        "      param.requires_grad = True\n",
        "    total_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(total_parameters)\n",
        "\n",
        "if use_lora:\n",
        "  assert(total_parameters == 1345552)\n",
        "else:\n",
        "  if fine_tune:\n",
        "    assert(total_parameters == 109483778)\n",
        "  else:\n",
        "    assert(total_parameters == 1538)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a36b60bf",
      "metadata": {
        "id": "a36b60bf"
      },
      "source": [
        "### Set the model to the right device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "071b1a84",
      "metadata": {
        "id": "071b1a84",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1a0191b-e01e-47c7-ff73-f10ef3009483"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "import platform\n",
        "\n",
        "# Pick the system you have and select GPU if you can\n",
        "if platform.system() == 'Darwin':\n",
        "    device = torch.device('mps') if torch.backends.mps.is_available() else torch.device('cpu')\n",
        "elif platform.system() == 'Linux':\n",
        "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4811b9f5",
      "metadata": {
        "id": "4811b9f5"
      },
      "outputs": [],
      "source": [
        "_ = model.to(device)\n",
        "\n",
        "# Recommended number of epochs: See: https://arxiv.org/pdf/1810.04805.pdf\n",
        "epochs = 2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Recommended learning rates (Adam): 5e-5, 3e-5, 2e-5\n",
        "# See: https://arxiv.org/pdf/1810.04805.pdf\n",
        "optimizer = torch.optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr = 3e-5,\n",
        "    eps = 1e-08\n",
        ")"
      ],
      "metadata": {
        "id": "nJ9A9Q6Zo9z8"
      },
      "id": "nJ9A9Q6Zo9z8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "765fc3a1",
      "metadata": {
        "id": "765fc3a1"
      },
      "source": [
        "### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf0b712e",
      "metadata": {
        "id": "bf0b712e"
      },
      "outputs": [],
      "source": [
        "# Use torchmetrics to set up accuracy, recall, precision, and auroc\n",
        "accuracy = Accuracy(task=\"binary\")\n",
        "recall = Recall(task=\"binary\")\n",
        "precision = Precision(task=\"binary\")\n",
        "auroc = AUROC(task=\"binary\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.device\n",
        "epochs = 2"
      ],
      "metadata": {
        "id": "Zw_zk3ZPpZXK"
      },
      "id": "Zw_zk3ZPpZXK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "084519f5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "084519f5",
        "outputId": "22f919da-4300-41c0-d6c5-58ab0fd6203d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A/usr/local/lib/python3.11/dist-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score\n",
            "  warnings.warn(*args, **kwargs)\n",
            "\n",
            "Epoch:  50%|█████     | 1/2 [00:23<00:23, 23.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\t - Train loss: 0.1469\n",
            "\t - Validation Accuracy: 0.9866\n",
            "\t - Validation Precision: 0.8583\n",
            "\t - Validation Recall: 0.8488\n",
            "\t - Validation AUROC: 0.9118\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch: 100%|██████████| 2/2 [00:45<00:00, 22.86s/it]\n",
            "100%|██████████| 2/2 [00:45<00:00, 22.86s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\t - Train loss: 0.0366\n",
            "\t - Validation Accuracy: 0.9857\n",
            "\t - Validation Precision: 0.8488\n",
            "\t - Validation Recall: 0.7983\n",
            "\t - Validation AUROC: 0.9122\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Main training / validation loop\n",
        "import tqdm\n",
        "\n",
        "for _ in tqdm.tqdm(trange(epochs, desc = 'Epoch')):\n",
        "\n",
        "    # ========== Training ==========\n",
        "\n",
        "    # Set model to training mode\n",
        "    model.train()\n",
        "\n",
        "    # Tracking variables\n",
        "    tr_loss = 0\n",
        "    nb_tr_examples, nb_tr_steps = 0, 0\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        batch = [t.to(device) for t in batch]\n",
        "        # Unpack the batch\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "        # Set the gradients to zero\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        train_output = model(\n",
        "            input_ids=b_input_ids,\n",
        "            attention_mask=b_input_mask,\n",
        "            labels=b_labels\n",
        "        )\n",
        "        loss = train_output.loss\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update tracking variables\n",
        "        tr_loss += loss.item()\n",
        "        nb_tr_examples += b_input_ids.size(0)\n",
        "        nb_tr_steps += 1\n",
        "\n",
        "    # ========== Validation ==========\n",
        "\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables\n",
        "    val_accuracy = []\n",
        "    val_precision = []\n",
        "    val_recall = []\n",
        "    val_auroc = []\n",
        "\n",
        "    for batch in validation_dataloader:\n",
        "        batch = [t.to(device) for t in batch]\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        with torch.no_grad():\n",
        "          # Forward pass\n",
        "            eval_output = model(\n",
        "                input_ids=b_input_ids,\n",
        "                attention_mask=b_input_mask\n",
        "            )\n",
        "            logits = eval_output.logits\n",
        "\n",
        "        # Calculate validation metrics\n",
        "        labels = b_labels.detach().cpu()\n",
        "        predicted_labels = torch.argmax(logits, dim=1).detach().cpu()\n",
        "        probs = torch.softmax(logits, dim=1)[:, 1].detach().cpu()\n",
        "\n",
        "        val_accuracy.append(accuracy(predicted_labels, labels).item())\n",
        "        val_recall.append(recall(predicted_labels, labels).item())\n",
        "        val_precision.append(precision(predicted_labels, labels).item())\n",
        "        val_auroc.append(auroc(probs, labels).item())\n",
        "\n",
        "    print('\\n\\t - Train loss: {:.4f}'.format(tr_loss / nb_tr_steps))\n",
        "    print('\\t - Validation Accuracy: {:.4f}'.format(sum(val_accuracy)/len(val_accuracy)))\n",
        "    print('\\t - Validation Precision: {:.4f}'.format(sum(val_precision)/len(val_precision)))\n",
        "    print('\\t - Validation Recall: {:.4f}'.format(sum(val_recall)/len(val_recall)))\n",
        "    print('\\t - Validation AUROC: {:.4f}\\n'.format(sum(val_auroc)/len(val_auroc)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13d64bc6",
      "metadata": {
        "id": "13d64bc6"
      },
      "source": [
        "### Test on a specific sentence, see the outcome"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6dfcd9e7",
      "metadata": {
        "id": "6dfcd9e7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2e8885b-75e3-4aec-fd60-4fd30a0d4ad7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Sentence:  WINNER!! As a valued network customer you have been selected to receivea £900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.\n",
            "Predicted Class:  Ham\n"
          ]
        }
      ],
      "source": [
        "new_sentence = 'WINNER!! As a valued network customer you have been selected to receivea £900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.'\n",
        "\n",
        "# We need Token IDs and Attention Mask for inference on the new sentence\n",
        "test_ids = []\n",
        "test_attention_mask = []\n",
        "\n",
        "# Apply the tokenizer\n",
        "encoding = preprocessing(new_sentence, tokenizer)\n",
        "\n",
        "# Extract IDs and Attention Mask\n",
        "test_ids.append(encoding['input_ids'])\n",
        "test_attention_mask.append(encoding['attention_mask'])\n",
        "test_ids = torch.cat(test_ids, dim = 0)\n",
        "test_attention_mask = torch.cat(test_attention_mask, dim = 0)\n",
        "\n",
        "# Forward pass, calculate logit predictions\n",
        "with torch.no_grad():\n",
        "    output = model(test_ids.to(device), token_type_ids = None, attention_mask = test_attention_mask.to(device))\n",
        "\n",
        "prediction = 'Spam' if np.argmax(output.logits.cpu().numpy()).flatten().item() == 1 else 'Ham'\n",
        "\n",
        "print('Input Sentence: ', new_sentence)\n",
        "print('Predicted Class: ', prediction)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f27c29b",
      "metadata": {
        "id": "5f27c29b"
      },
      "source": [
        "### Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c82ced0",
      "metadata": {
        "id": "8c82ced0"
      },
      "source": [
        "Question 1: Run the above by fine tuning bert and the classfier head and by not doing this (using BERT as a feature encoder). What is the gap between this?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If Fine Tune\n",
        "import tqdm\n",
        "\n",
        "for _ in tqdm.tqdm(trange(epochs, desc = 'Epoch')):\n",
        "\n",
        "    # ========== Training ==========\n",
        "\n",
        "    # Set model to training mode\n",
        "    model.train()\n",
        "\n",
        "    # Tracking variables\n",
        "    tr_loss = 0\n",
        "    nb_tr_examples, nb_tr_steps = 0, 0\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        batch = [t.to(device) for t in batch]\n",
        "        # Unpack the batch\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "        # Set the gradients to zero\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        train_output = model(\n",
        "            input_ids=b_input_ids,\n",
        "            attention_mask=b_input_mask,\n",
        "            labels=b_labels\n",
        "        )\n",
        "        loss = train_output.loss\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update tracking variables\n",
        "        tr_loss += loss.item()\n",
        "        nb_tr_examples += b_input_ids.size(0)\n",
        "        nb_tr_steps += 1\n",
        "\n",
        "    # ========== Validation ==========\n",
        "\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables\n",
        "    val_accuracy = []\n",
        "    val_precision = []\n",
        "    val_recall = []\n",
        "    val_auroc = []\n",
        "\n",
        "    for batch in validation_dataloader:\n",
        "        batch = [t.to(device) for t in batch]\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        with torch.no_grad():\n",
        "          # Forward pass\n",
        "            eval_output = model(\n",
        "                input_ids=b_input_ids,\n",
        "                attention_mask=b_input_mask\n",
        "            )\n",
        "            logits = eval_output.logits\n",
        "\n",
        "        # Calculate validation metrics\n",
        "        labels = b_labels.detach().cpu()\n",
        "        predicted_labels = torch.argmax(logits, dim=1).detach().cpu()\n",
        "        probs = torch.softmax(logits, dim=1)[:, 1].detach().cpu()\n",
        "\n",
        "        val_accuracy.append(accuracy(predicted_labels, labels).item())\n",
        "        val_recall.append(recall(predicted_labels, labels).item())\n",
        "        val_precision.append(precision(predicted_labels, labels).item())\n",
        "        val_auroc.append(auroc(probs, labels).item())\n",
        "\n",
        "    print('\\n\\t - Train loss: {:.4f}'.format(tr_loss / nb_tr_steps))\n",
        "    print('\\t - Validation Accuracy: {:.4f}'.format(sum(val_accuracy)/len(val_accuracy)))\n",
        "    print('\\t - Validation Precision: {:.4f}'.format(sum(val_precision)/len(val_precision)))\n",
        "    print('\\t - Validation Recall: {:.4f}'.format(sum(val_recall)/len(val_recall)))\n",
        "    print('\\t - Validation AUROC: {:.4f}\\n'.format(sum(val_auroc)/len(val_auroc)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNgSZv8-qKPD",
        "outputId": "8bf1d0f0-2312-4e1d-bede-672b5f545e80"
      },
      "id": "aNgSZv8-qKPD",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A/usr/local/lib/python3.11/dist-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score\n",
            "  warnings.warn(*args, **kwargs)\n",
            "\n",
            "Epoch:  50%|█████     | 1/2 [00:32<00:32, 32.55s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\t - Train loss: 0.0795\n",
            "\t - Validation Accuracy: 0.9821\n",
            "\t - Validation Precision: 0.8400\n",
            "\t - Validation Recall: 0.8524\n",
            "\t - Validation AUROC: 0.9086\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch: 100%|██████████| 2/2 [01:05<00:00, 32.84s/it]\n",
            "100%|██████████| 2/2 [01:05<00:00, 32.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\t - Train loss: 0.0288\n",
            "\t - Validation Accuracy: 0.9884\n",
            "\t - Validation Precision: 0.8781\n",
            "\t - Validation Recall: 0.8440\n",
            "\t - Validation AUROC: 0.9114\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If nothing\n",
        "import tqdm\n",
        "\n",
        "for _ in tqdm.tqdm(trange(epochs, desc = 'Epoch')):\n",
        "\n",
        "    # ========== Training ==========\n",
        "\n",
        "    # Set model to training mode\n",
        "    model.train()\n",
        "\n",
        "    # Tracking variables\n",
        "    tr_loss = 0\n",
        "    nb_tr_examples, nb_tr_steps = 0, 0\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        batch = [t.to(device) for t in batch]\n",
        "        # Unpack the batch\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "        # Set the gradients to zero\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        train_output = model(\n",
        "            input_ids=b_input_ids,\n",
        "            attention_mask=b_input_mask,\n",
        "            labels=b_labels\n",
        "        )\n",
        "        loss = train_output.loss\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update tracking variables\n",
        "        tr_loss += loss.item()\n",
        "        nb_tr_examples += b_input_ids.size(0)\n",
        "        nb_tr_steps += 1\n",
        "\n",
        "    # ========== Validation ==========\n",
        "\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables\n",
        "    val_accuracy = []\n",
        "    val_precision = []\n",
        "    val_recall = []\n",
        "    val_auroc = []\n",
        "\n",
        "    for batch in validation_dataloader:\n",
        "        batch = [t.to(device) for t in batch]\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        with torch.no_grad():\n",
        "          # Forward pass\n",
        "            eval_output = model(\n",
        "                input_ids=b_input_ids,\n",
        "                attention_mask=b_input_mask\n",
        "            )\n",
        "            logits = eval_output.logits\n",
        "\n",
        "        # Calculate validation metrics\n",
        "        labels = b_labels.detach().cpu()\n",
        "        predicted_labels = torch.argmax(logits, dim=1).detach().cpu()\n",
        "        probs = torch.softmax(logits, dim=1)[:, 1].detach().cpu()\n",
        "\n",
        "        val_accuracy.append(accuracy(predicted_labels, labels).item())\n",
        "        val_recall.append(recall(predicted_labels, labels).item())\n",
        "        val_precision.append(precision(predicted_labels, labels).item())\n",
        "        val_auroc.append(auroc(probs, labels).item())\n",
        "\n",
        "    print('\\n\\t - Train loss: {:.4f}'.format(tr_loss / nb_tr_steps))\n",
        "    print('\\t - Validation Accuracy: {:.4f}'.format(sum(val_accuracy)/len(val_accuracy)))\n",
        "    print('\\t - Validation Precision: {:.4f}'.format(sum(val_precision)/len(val_precision)))\n",
        "    print('\\t - Validation Recall: {:.4f}'.format(sum(val_recall)/len(val_recall)))\n",
        "    print('\\t - Validation AUROC: {:.4f}\\n'.format(sum(val_auroc)/len(val_auroc)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJ_TYPkTzBni",
        "outputId": "97e66480-7998-4de6-dbe8-85368dfd680e"
      },
      "id": "iJ_TYPkTzBni",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch:   0%|          | 0/2 [00:00<?, ?it/s]\n",
            "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch:  50%|█████     | 1/2 [00:09<00:09,  9.88s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\t - Train loss: 0.4351\n",
            "\t - Validation Accuracy: 0.8648\n",
            "\t - Validation Precision: 0.0000\n",
            "\t - Validation Recall: 0.0000\n",
            "\t - Validation AUROC: 0.6092\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch: 100%|██████████| 2/2 [00:20<00:00, 10.31s/it]\n",
            "100%|██████████| 2/2 [00:20<00:00, 10.31s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\t - Train loss: 0.3818\n",
            "\t - Validation Accuracy: 0.8657\n",
            "\t - Validation Precision: 0.0000\n",
            "\t - Validation Recall: 0.0000\n",
            "\t - Validation AUROC: 0.7892\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dae9055d",
      "metadata": {
        "id": "dae9055d"
      },
      "source": [
        "When neither LoRA is used nor BERT is fine-tuned, the model updates only the classifier head, totaling just 1,538 trainable parameters. This configuration trains extremely fast due to the small number of parameters. However, performance is significantly limited, with a validation accuracy of only 86.66%, reflecting the model’s inability to adapt BERT’s internal representations.\n",
        "\n",
        "In contrast, when we fully fine-tune BERT, the number of trainable parameters jumps to 109,483,778, encompassing the entire model. While this results in a much longer training time, even for only 2 epochs, the performance improves substantially, achieving a validation accuracy of 98.84%.\n",
        "\n",
        "Using LoRA offers a strong trade-off. With just 1,345,552 trainable parameters, roughly 1% of full fine-tuning, we obtain an accuracy of 98.57%, very close to full fine-tuning with only a 0.3% gap. This demonstrates that LoRA is highly effective in adapting large models efficiently while dramatically reducing computational costs."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.15"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}